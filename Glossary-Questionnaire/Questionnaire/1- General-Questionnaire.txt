1- How do humans interact with light?

Ans- Humans capture light through the eyes, which convert it into electrical signals processed by the brain to form visual images.

(------------------------------------------------------------------------)

2- What is vision, and why is it significant in human evolution?

Ans- Vision is the process of capturing and interpreting light, which was crucial in developing the complex nervous systems and large brains of humans.

(-------------------------------------------------------------------- ---)

3- How does the brain process visual information to perform actions, like kicking a ball?

Ans- The brain identifies objects, predicts their movement, and coordinates muscle actions in a split second, all based on visual inputs.

(------------------------------------------------------------------------)

4- Why don't humans need formal education to perform everyday tasks like kicking a ball?

Ans- Humans learn these tasks through trial and error during their upbringing, relying on innate cognitive abilities rather than formal education.

(------------------------------------------------------------------------)

5- What challenges arise when programming a computer to recognize a ball?

Ans- Defining what constitutes a ball is difficult due to variations in size, shape, and context, making rule-based programming challenging.

(------------------------------------------------------------------------)

6- Why can't rigid rule-based systems fully replicate human object recognition?

Ans- Human object recognition relies on context and generalization, whereas rigid systems lack the flexibility to adapt to different scenarios.

(------------------------------------------------------------------------)

7- How does computer vision differ from human vision?

Ans- Computer vision involves algorithms and models that process visual data differently from human vision, often focusing on specific tasks.

(------------------------------------------------------------------------)

8- Why is adaptability important in computer vision systems?

Ans- Adaptability allows computer vision systems to handle varied scenarios and context-rich environments, similar to human perception.

(------------------------------------------------------------------------)

9- What are some practical applications of computer vision in sports?

Ans- Computer vision can track balls in sports for real-time decisions and make events more accessible to those with visual impairments.

(------------------------------------------------------------------------)

10- How has the AI renaissance impacted computer vision?

Ans- The AI renaissance has enabled the creation of advanced computer vision models that can detect, generate, and describe images more effectively.

(------------------------------------------------------------------------)

11- What is an image in the context of computer vision?

Ans- An image is a visual representation of objects, scenes, or concepts, typically represented as a 2D function F(X,Y) where X and Y are spatial Coordinates.

(------------------------------------------------------------------------)

12- How is an image represented mathematically?

Ans- An image is represented as an n-dimensional function F(X,Y) with the function value indicating the intensity or gray level at a specific point.

(------------------------------------------------------------------------)

13- What do X and Y represent in an image function F(X,Y)?

Ans- X and Y represent the spatial coordinates of the image, often in a 2D Cartesian system.

(------------------------------------------------------------------------)

14- What is the significance of intensity in an image?

Ans- Intensity indicates the level of light and dark in an image, corresponding to the amplitude of the function at specific coordinates.

(------------------------------------------------------------------------)

15- What is a pixel in the context of images?

Ans- A pixel, or picture element, is the smallest unit of an image, representing the intensity at a specific coordinate pair (X,Y)

(------------------------------------------------------------------------)

16- How are 3D images represented?

Ans- 3D images are represented by the function F(X,Y,Z), where X, Y, and Z are spatial coordinates, and each point is referred to as a voxel(volume element)

(------------------------------------------------------------------------)

17- What is a voxel?

Ans- A voxel is the 3D equivalent of a pixel, representing a value in a volumetric image at coordinates (X, Y, Z)

(------------------------------------------------------------------------)

18- What are image channels?

Ans- Image channels refer to the different color components of an image, with each channel representing intensity levels of a specific color, like red, green, or blue.

(------------------------------------------------------------------------)

19- What is the range of intensity values in a single color channel?

Ans- The intensity values in a color channel typically range from 0 to 255, where 0 represents no intensity and 255 represents maximum intensity.

(------------------------------------------------------------------------)

20- What is a labeled image?

Ans- A labeled image assigns labels to pixels, often used to differentiate foreground and background, with binary images assigning values like 0 and 1.

(------------------------------------------------------------------------)

21- What is the difference between a 2D image and a video?

Ans- A 2D image is a static representation at a single point in time, while a video is a sequence of images over time, represented by the function F(X,Y,T).

(------------------------------------------------------------------------)

22- How are images typically represented in computers?

Ans- Images are commonly represented as matrices, where each element corresponds to a pixel's intensity value in a 2D array.

(------------------------------------------------------------------------)

23- Can images be represented as graphs?

Ans- Yes, images can be represented as graphs where nodes represent coordinates and edges represent neighboring coordinates.

(------------------------------------------------------------------------)

24- What is the difference between images and tabular data in terms of dimensionality?

Ans- In images, dimensionality refers to spatial dimensions (e.g., width and height), while in tabular data, it refers to the number of features (columns) describing each data point.

(------------------------------------------------------------------------)

25- What is feature extraction in the context of images?

Ans- Feature extraction for images involves identifying important characteristics like edges, textures, and colors, which are used for further analysis.

(------------------------------------------------------------------------)

26- What are the key differences between images and other data types like audio or tabular data?

Ans- Images are visual data often represented as 2D arrays, videos add a time dimension, audio is typically 1D, and tabular data is structured in rows and columns.

(------------------------------------------------------------------------)

27- What are some common machine learning tasks associated with images?

Ans- Common tasks include image classification, segmentation, and object detection.

(------------------------------------------------------------------------)

28- How do computational costs compare between processing images, videos, and tabular data?

Ans- Image processing is generally less expensive than video processing, while tabular data processing is usually less expensive compared to both images and videos.

(------------------------------------------------------------------------)

29- What are some applications of image processing?

Ans- Applications include facial recognition, object detection, and image segmentation.

(------------------------------------------------------------------------)

30- What file types are commonly used for images?

Ans- Common image file types include JPEG, PNG, and RAW.

(------------------------------------------------------------------------)

31- What is the first step in digital image processing?

Ans- Image acquisition is the first step, involving the capture of physical phenomena into a digital representation.

(------------------------------------------------------------------------)

32- What types of energy can be used in image acquisition?

Ans- Energy types include conventional light, electromagnetic waves, and ultrasound.

(------------------------------------------------------------------------)

33- How does a sensor contribute to image acquisition?

Ans- Sensors convert incident energy into electrical signals, which are then digitized into an image.

(------------------------------------------------------------------------)

34- What is the role of sensor arrays like CCDs in digital cameras?

Ans- CCDs capture a complete image simultaneously without the need for motion.

(------------------------------------------------------------------------)

35- What are the key components in digital image formation?

Ans- Key components are illumination, reflectance, sampling, and quantization.

(------------------------------------------------------------------------)

36- What factors influence the resolution and quality of a digital image?

Ans- Resolution and quality depend on the number of samples, discrete intensity levels, and the system's dynamic range.

(------------------------------------------------------------------------)

37- What does spatial resolution refer to in digital imaging?

Ans- Spatial resolution refers to the smallest distinguishable detail in an image.

(------------------------------------------------------------------------)

38- How is intensity resolution typically quantized?

Ans- Intensity resolution is often quantized in binary increments, such as 8-bit or 256 levels.

(------------------------------------------------------------------------)

39- What is the difference between image restoration and image enhancement?

Ans- Restoration aims to recover a degraded image, while enhancement improves visual appearance.

(------------------------------------------------------------------------)

40- Why is color important in image processing?

Ans- Color helps in object identification and recognition, utilizing both pseudo-color and full-color processing.

(------------------------------------------------------------------------)

41- What is the RGB color model used for?

Ans- The RGB color model is used for digital displays, representing images with red, green, and blue components.

(------------------------------------------------------------------------)

42- What is the primary goal of image compression?

Ans- Image compression aims to reduce data redundancy to efficiently represent information.

(------------------------------------------------------------------------)

43- What causes coding redundancy in digital images?

Ans- Coding redundancy arises when intensity values are not uniformly distributed, leading to inefficient use of bits.

(------------------------------------------------------------------------)

44- How does run-length encoding reduce spatial redundancy?

Ans- Run-length encoding compresses data by reducing the redundancy in images with constant intensity lines.

(------------------------------------------------------------------------)

45- What is the significance of Huffman coding in image compression?

Ans- Huffman coding removes coding redundancy by efficiently representing source symbols based on their probability.

(------------------------------------------------------------------------)

46- Why might a higher resolution camera not solve all imaging problems?

Ans- Higher resolution can introduce more noise, require more resources, and may not be suitable for all applications.

(------------------------------------------------------------------------)

47- How does image resolution affect model training in machine learning?

Ans- Higher resolution requires more computational resources, longer training times, and may reduce the number of images that can be processed.

(------------------------------------------------------------------------)

48- How has imaging technology evolved to capture what the human eye cannot see?

Ans- Technologies like infrared, magnetic resonance, and electron microscopy allow us to image beyond the visible spectrum.

(------------------------------------------------------------------------)

49- What is the significance of Picture 51 in the history of imaging?

Ans- Picture 51 was crucial in the discovery of DNA's double helix structure.

(------------------------------------------------------------------------)

50- What challenges were faced in capturing the first image of a black hole?

Ans- Capturing the black hole required synchronizing a global network of telescopes to create a massive virtual telescope.

(------------------------------------------------------------------------)

51- What innovative method was used to store images in DNA?

Ans- Scientists encoded images into DNA using CRISPR, demonstrating a novel way to archive data.

(------------------------------------------------------------------------)

52- What is computer vision?

Ans- Computer vision is the science and technology of making machines see, involving methods to acquire, process, analyze, and understand visual data.

(------------------------------------------------------------------------)

53- What is the difference between image analysis and image understanding?

Ans- Image analysis focuses on low and mid-level processes, while image understanding involves making sense of the entire image, often through higher-level cognitive tasks.

(------------------------------------------------------------------------)

54- What are the three levels of image understanding?

Ans- The three levels are low-level (basic image operations), mid-level (segmentation and classification), and high-level (object recognition and scene reconstruction).

(------------------------------------------------------------------------)

55- What factors can increase the complexity of a computer vision task?

Ans- Factors include lighting conditions, occlusions, image resolution, and camera quality.

(------------------------------------------------------------------------)

56- What are some applications of computer vision?

Ans- Applications include image captioning, anomaly detection, image restoration, and autonomous exploration.

(------------------------------------------------------------------------)

57- What ethical concerns arise with computer vision applications?

Ans- Concerns include privacy issues in surveillance and the potential for misclassification in critical tasks like medical diagnosis.

(------------------------------------------------------------------------)

58- What role does image preprocessing play in computer vision?

Ans- Image preprocessing helps extract features and improve data quality before applying more complex models or machine learning algorithms.

(------------------------------------------------------------------------)

59- What is scene recognition in computer vision?

Ans- Scene recognition involves identifying and understanding the overall environment or context within an image.

(------------------------------------------------------------------------)

60- What is an example of a low-level image understanding task?

Ans- An example is image sharpening, where the input and output are both images.

(------------------------------------------------------------------------)

61- What is a high-level image understanding task?

Ans- A high-level task could be recognizing a specific object or scene in an image, akin to human cognition.

(------------------------------------------------------------------------)

62- How can classical image analysis methods contribute to modern computer vision?

Ans- Classical methods can aid in data augmentation, improving the quality and diversity of training data for contemporary computer vision models.

(------------------------------------------------------------------------)

63-  What are some challenges in pedestrian detection using computer vision?

Ans- Challenges include varying lighting conditions, occlusions, and low-resolution images, which complicate the detection process.

(------------------------------------------------------------------------)

64- What role does computer vision play in autonomous vehicles?

Ans- It helps vehicles perceive and interpret their surroundings for real-time decision-making.

(------------------------------------------------------------------------)

65- How is computer vision utilized in retail and e-commerce?

Ans- It powers object recognition, recommendation systems, and inventory tracking.

(------------------------------------------------------------------------)

66- Can you explain how computer vision enhances customer experience in online shopping?

Ans- It suggests similar products by analyzing images or videos of items viewed or purchased by customers.

(------------------------------------------------------------------------)

67- How does computer vision assist in tumor detection in radiology?

Ans- It segments and detects tumors in MRI or CT scans, aiding in diagnosis and treatment planning.

(------------------------------------------------------------------------)

68- What are some common challenges faced by computer vision systems?

Ans- Issues include data variability, scalability, accuracy, and robustness to noise.

(------------------------------------------------------------------------)

69- How do privacy concerns impact the deployment of computer vision systems?

Ans- They raise ethical issues, particularly in surveillance and facial recognition applications.

(------------------------------------------------------------------------)

70- Why is bias a significant concern in computer vision systems?

Ans- Biases can lead to unfair outcomes, perpetuating social inequalities.

(------------------------------------------------------------------------)

71- What is the importance of transparency in computer vision systems?

Ans- Users need to understand how these systems make decisions to ensure accountability and trust.

(------------------------------------------------------------------------)

72- What are the main categories of operations in digital image processing?

Ans- Logical, Statistical, Geometrical, Mathematical, and Transform operations.

(------------------------------------------------------------------------)

73- What is the purpose of logical operations in image processing?

Ans- Logical operations apply rules to image pixels to produce new images, often used in binary image manipulation.

(------------------------------------------------------------------------)

74- Can you explain the difference between element-wise and matrix operations in image processing?

Ans- Element-wise operations process each pixel individually, while matrix operations manipulate the entire image using matrix theory.

(------------------------------------------------------------------------)

75- How is set theory applied in digital image processing?

Ans- Set theory helps perform operations like union and intersection on binary images to analyze pixel relationships.

(------------------------------------------------------------------------)

76- What is the role of intensity transformations in image processing?

Ans- Intensity transformations adjust pixel values to enhance image contrast or brightness.

(------------------------------------------------------------------------)

77- How does spatial filtering enhance an image?

Ans- Spatial filtering modifies pixel values based on their neighbors, improving features like sharpness or reducing noise.

(------------------------------------------------------------------------)

78- What is a linear spatial filter, and what are its types?

Ans- A linear spatial filter applies a convolution operation, with types including low pass (blurring) and high pass (sharpening) filters.

(------------------------------------------------------------------------)

79- What are the applications of Gaussian and box filters?

Ans- Gaussian filters are used for smoothing images, while box filters are simpler and used for basic blurring.

(------------------------------------------------------------------------)

80- How do sharpening filters like the Laplacian work?

Ans- Sharpening filters emphasize intensity transitions by enhancing edges and details in an image.

(------------------------------------------------------------------------)

81- What is data augmentation in the context of image processing?

Ans- Data augmentation involves creating modified versions of existing data to increase the diversity and size of a training dataset.

(------------------------------------------------------------------------)

82- How does data augmentation benefit Convolutional Neural Networks (CNNs)?

Ans- It improves model performance by preventing overfitting and enhancing generalization through varied training data.

(------------------------------------------------------------------------)

83- What are some common image augmentation techniques?

Ans- Techniques include flipping, cropping, rotation, brightness adjustment, and applying kernel filters.

(------------------------------------------------------------------------)

84- How is synthetic data different from augmented data?

Ans- Synthetic data is entirely generated from scratch, often using techniques like GANs, while augmented data is created by modifying existing data.

(------------------------------------------------------------------------)

85- Why is data augmentation important in scenarios with limited training data?

Ans- It helps in expanding the dataset and improving model accuracy without the need for additional data collection.

(------------------------------------------------------------------------)

86- What are the challenges associated with data augmentation?

Ans- Challenges include maintaining quality assurance and avoiding the persistence of biases present in the original dataset.

(------------------------------------------------------------------------)

87- How does data augmentation reduce the cost of data labeling?

Ans- By generating varied data points, it decreases the need for manual labeling, saving time and resources.

(------------------------------------------------------------------------)

88- What tools are commonly used for image data augmentation?

Ans- Tools like PyTorch, Augmentor, Albumentations, Imgaug, and OpenCV are popular for implementing various augmentations.

(------------------------------------------------------------------------)

89- How does batch-wise augmentation during model training conserve disk space?

Ans- It generates transformed images on-the-fly during training, avoiding the need to store them permanently.

(-------------------------------------------------------------------------)

90- What are features in the context of machine learning?

Ans- Features are attributes or variables of instances learned by the model to help recognize new instances.

(-------------------------------------------------------------------------)

91- How can numerical features be represented in data structures?

Ans- Numerical features can be represented as arrays/lists or tensors for efficient handling.

(-------------------------------------------------------------------------)

92- What data structures are used to represent categorical features?

Ans- Categorical features can be represented using dictionaries/lists or one-hot encoding.

(-------------------------------------------------------------------------)

93- How are image features commonly represented in data?

Ans- Image features are represented as pixel value matrices or using features extracted by CNNs.

(-------------------------------------------------------------------------)

94- What makes a good descriptor in image processing?

Ans- A good descriptor is invariant to transformations, distinctive, and efficient.

(-------------------------------------------------------------------------)

95- Why is invariance important in a descriptor?

Ans- Invariance ensures that the descriptor remains consistent despite changes in rotation, scale, or illumination.

(-------------------------------------------------------------------------)

96- What does distinctiveness in a descriptor refer to?

Ans- Distinctiveness means the descriptor can uniquely identify and differentiate objects or image parts.

(-------------------------------------------------------------------------)

97- Why is the dimensionality of a descriptor important?

Ans- Balancing dimensionality is crucial for efficient processing and storage while conveying sufficient information.

(-------------------------------------------------------------------------)

98- What role does locality play in a good descriptor?

Ans- Locality ensures that the descriptor captures specific regions or keypoints within an image.

(-------------------------------------------------------------------------)

99- What is meant by the repeatability of a descriptor?

Ans- Repeatability refers to the consistency of a descriptor across different instances of the same object or scene.

(-------------------------------------------------------------------------)

100- How should descriptors be compatible with matching algorithms?

Ans- Descriptors should be suitable for the specific matching algorithm, whether based on distance metrics or other techniques.

(-------------------------------------------------------------------------)

101- Why is computational efficiency important for descriptors?

Ans- Computational efficiency is critical for real-time applications where quick processing is needed.

(-------------------------------------------------------------------------)

102- What is adaptability in the context of descriptors?

Ans- Adaptability allows descriptors to adjust or learn from the data, making them effective in dynamic environments.

(-------------------------------------------------------------------------)

103- How does noise robustness contribute to a descriptor's quality?

Ans- Noise robustness ensures that a descriptor can handle image noise without losing accuracy.

(-------------------------------------------------------------------------)

104- What is SIFT in computer vision?

Ans- SIFT (Scale-Invariant Feature Transform) is an algorithm used to detect and describe local features in images.

(-------------------------------------------------------------------------)

105- What is the basic working principle of SIFT?

Ans- SIFT detects keypoints across scales, assigns orientations, and generates descriptors for matching.

(-------------------------------------------------------------------------)

106- What is the key strength of SIFT?

Ans- SIFT's robustness to transformations makes it valuable for tasks like object recognition and image stitching.

(-------------------------------------------------------------------------)

107- What does SURF stand for in computer vision?

Ans- SURF stands for Speeded Up Robust Features, known for its speed in detecting and describing local image features.

(-------------------------------------------------------------------------)

108- How does SURF improve computational efficiency?

Ans- SURF uses integral images and Haar wavelet approximations for faster feature computation.

(-------------------------------------------------------------------------)

109- What is the basic working principle of SURF?

Ans- SURF detects blobs using the Hessian matrix, assigns orientations, and matches descriptors across images.

(-------------------------------------------------------------------------)

110- What is the key advantage of SURF over SIFT?

Ans- SURF is computationally more efficient, making it suitable for real-time applications.

(-------------------------------------------------------------------------)

111- What is feature matching in computer vision?

Ans- Feature matching involves comparing key attributes in different images to identify similarities.

(-------------------------------------------------------------------------)

112- What is the purpose of feature matching in computer vision applications?

Ans- It is used in scene understanding, image stitching, object tracking, and pattern recognition.

(-------------------------------------------------------------------------)

113- What is Brute-Force Search in feature matching?

Ans- Brute-Force Search compares every pixel or descriptor from one image to every pixel or descriptor in another, ensuring exhaustive matching.

(-------------------------------------------------------------------------)

114- What are the advantages and disadvantages of Brute-Force Search?

Ans- Advantage: Simplicity; Disadvantage: Time-consuming for large datasets.

(-------------------------------------------------------------------------)

115- How does SIFT work in feature matching?

Ans- SIFT detects keypoints and descriptors in images, which are then matched using methods like Brute-Force or FLANN.

(-------------------------------------------------------------------------)

116- What is the role of k nearest neighbors (k-NN) in SIFT-based feature matching?

Ans- k-NN helps find the closest matches for descriptors between two images.

(-------------------------------------------------------------------------)

117- What is the purpose of the ratio test in feature matching?

Ans- The ratio test filters out weak matches by comparing the distances of the nearest neighbors.

(-------------------------------------------------------------------------)

118- How does ORB differ from SIFT in feature matching?

Ans- ORB uses binary descriptors and matches features using Hamming Distance, making it faster and more efficient.

(-------------------------------------------------------------------------)

119- What is Hamming Distance, and when is it used?

Ans- Hamming Distance measures the difference between two binary strings, commonly used in ORB-based matching.

(-------------------------------------------------------------------------)

120- What is FLANN, and how does it improve feature matching?

Ans- FLANN (Fast Library for Approximate Nearest Neighbors) uses k-D trees for faster matching by approximating nearest neighbors.

(-------------------------------------------------------------------------)

121- What are k-D trees, and why are they used in FLANN?

Ans- k-D trees organize data points for efficient nearest-neighbor searches, reducing the number of comparisons needed.

(-------------------------------------------------------------------------)

122- How does FLANN adjust its strategy for different types of features?

Ans- FLANN customizes its approach based on the features, focusing on color, shape, or other attributes as needed.

(-------------------------------------------------------------------------)

123- What is LoFTR, and how does it differ from traditional feature matching methods?

Ans- LoFTR uses transformers and a learning-based approach to match features without relying on traditional detectors.

(-------------------------------------------------------------------------)

124- How does LoFTR handle image transformations like rotation and scaling?

Ans- LoFTR is robust to transformations, maintaining accuracy even when features are rotated or resized.

(-------------------------------------------------------------------------)

125- Why is RANSAC used in LoFTR, and what is its purpose?

Ans- RANSAC is used to clean up correspondences by filtering outliers, ensuring reliable feature matches.

(-------------------------------------------------------------------------)

126- What libraries are essential for implementing feature matching in Python?

Ans- Libraries like OpenCV, Kornia, and torch are essential for feature matching implementation.

(-------------------------------------------------------------------------)

127- How does Kornia facilitate feature matching in Python?

Ans- Kornia provides tools for loading images, converting them to grayscale, and applying matching algorithms like LoFTR.

(-------------------------------------------------------------------------)

128- Why is it important to convert images to grayscale in LoFTR-based matching?

Ans- LoFTR operates on grayscale images to focus on structure and intensity variations, which are critical for feature matching.

(-------------------------------------------------------------------------)

129- What is the significance of the Random Sample Consensus (RANSAC) in feature matching?

Ans- RANSAC helps in identifying the inliers among matches, improving the reliability of the matching process.

(-------------------------------------------------------------------------)

130- How do you visualize feature matches between two images?

Ans- Matches can be visualized using functions like `cv2.drawMatches` or `draw_LAF_matches` in Python.

(-------------------------------------------------------------------------)

131- What is feature extraction in computer vision?

Ans- Feature extraction is the process of identifying and isolating relevant visual information from images or videos for analysis by machines.

(-------------------------------------------------------------------------)

132- What role does feature extraction play in facial recognition?

Ans- It identifies unique facial features like distances between eyes, nose shape, and jawline contours, crucial for accurate recognition.

(-------------------------------------------------------------------------)

133- Describe an application of facial recognition in healthcare.

Ans- Tools like Face2Gene analyze facial features to help diagnose genetic conditions.

(-------------------------------------------------------------------------)

134- How do marketing and retail industries utilize facial recognition?

Ans- They use it to gauge customer reactions to products or ads, enabling them to adapt strategies based on emotional responses.

(-------------------------------------------------------------------------)

135- What is object tracking in computer vision?

Ans- Object tracking involves continuously detecting and following key features of an object across video frames.

(-------------------------------------------------------------------------)

136- How is object tracking applied in automotive safety?

Ans- Tesla’s Autopilot uses object tracking to monitor surrounding vehicles, enhancing driving safety.

(-------------------------------------------------------------------------)

137- Name a sports-related application of object tracking.

Ans- Hawk-Eye technology in sports like tennis tracks ball movement to aid in accurate decision-making.

(-------------------------------------------------------------------------)

138- What is anomaly detection in the context of visual data?

Ans- It identifies patterns in visual data that deviate from the norm, often using techniques ranging from statistical methods to neural networks.

(-------------------------------------------------------------------------)

139- How is anomaly detection used in public safety?

Ans- It helps identify suspicious activities or abandoned objects in urban surveillance, such as in London’s CCTV network.

(-------------------------------------------------------------------------)

140- Give an example of anomaly detection in industrial quality control.

Ans- BMW uses it to detect defects in car parts during production.

(-------------------------------------------------------------------------)

141- What is the role of anomaly detection in healthcare diagnostics?

Ans- It aids in identifying tumors or abnormalities in medical imaging, with AI-driven platforms assisting radiologists.

(-------------------------------------------------------------------------)

142- Why is feature extraction considered transformative in computer vision?

Ans- It underpins a wide range of applications, enhancing security, medical diagnostics, and industrial monitoring, among others.

(-------------------------------------------------------------------------)

143- How might the scope of feature extraction evolve with advancing technology?

Ans- As technology progresses, feature extraction will offer more sophisticated solutions across diverse sectors, driving future innovations.

(-------------------------------------------------------------------------)

144- What are pixels in an image?

Ans- Pixels are the smallest units of an image, representing color and intensity at a specific location.

(-------------------------------------------------------------------------)

145- How is a digital image represented?

Ans- A digital image is represented as a matrix of pixel values.

(-------------------------------------------------------------------------)

146- What is the difference between grayscale and color images?

Ans- Grayscale images have one channel representing intensity, while color images have three channels (RGB) representing red, green, and blue intensities.

(-------------------------------------------------------------------------)

147- What are image channels?

Ans- Channels are separate layers in an image, like red, green, and blue in RGB images, each holding intensity values.

(-------------------------------------------------------------------------)

148- What are visual features in the context of computer vision?

Ans- Visual features are specific patterns or characteristics in an image, such as edges, corners, textures, or shapes.

(-------------------------------------------------------------------------)

149- What is edge detection?

Ans- Edge detection is the process of identifying and locating sharp discontinuities in an image, which represent object boundaries.

(-------------------------------------------------------------------------)

150- Name two popular edge detection methods.

Ans- Sobel and Canny edge detection methods.

(-------------------------------------------------------------------------)

151- What is the role of a kernel in image processing?

Ans- A kernel, or filter, is used to modify the image by performing convolution to extract features like edges or textures.

(-------------------------------------------------------------------------)

152- What is convolution in image processing?

Ans- Convolution is an operation where a kernel moves over an image to compute the sum of element-wise products, producing a feature map.

(-------------------------------------------------------------------------)

153- What is the importance of padding in convolution?

Ans- Padding adds extra pixels around the image border to ensure that the convolution operation covers the entire image.

(-------------------------------------------------------------------------)

154- What is stride in convolution?

Ans- Stride is the number of pixels the kernel moves over the image during the convolution process.

(-------------------------------------------------------------------------)

155- What is a feature map?

Ans- A feature map is the output of a convolutional layer, highlighting the detected features in the image.

(-------------------------------------------------------------------------)

156- What is the difference between a Prewitt and a Sobel filter?

Ans- Both are edge detection filters, but the Sobel filter gives more emphasis to edges by weighting the center pixel more heavily.

(-------------------------------------------------------------------------)

157- Why are CNNs better suited for image processing compared to classical methods?

Ans- CNNs automatically learn to detect relevant features from large datasets, outperforming manual feature extraction methods.

(-------------------------------------------------------------------------)

158- How does a convolutional layer in a CNN work?

Ans- It applies a set of learnable filters to the input image to produce feature maps that highlight specific patterns.

(-------------------------------------------------------------------------)

159- What is pooling in CNNs?

Ans- Pooling is a downsampling operation that reduces the spatial dimensions of feature maps, keeping only the most significant information.

(-------------------------------------------------------------------------)

160- What is max pooling?

Ans- Max pooling selects the maximum value from each window of a feature map, reducing the dimensionality while preserving important features.

(-------------------------------------------------------------------------)

161- What is average pooling?

Ans- Average pooling computes the average value from each window of a feature map, often used to reduce feature map size while smoothing.

(-------------------------------------------------------------------------)

162- What is a fully connected layer in CNNs?

Ans- A fully connected layer connects every neuron from the previous layer to every neuron in the current layer, usually used for classification.

(-------------------------------------------------------------------------)

163- What is dropout in CNNs?

Ans- Dropout randomly sets a fraction of input units to zero during training to prevent overfitting.

(-------------------------------------------------------------------------)

164- How does backpropagation work in CNNs?

Ans- Backpropagation adjusts the weights of the filters by computing the gradient of the loss function with respect to each weight and updating them.

(-------------------------------------------------------------------------)

165- What is the significance of parameter sharing in CNNs?

Ans- Parameter sharing reduces the number of parameters by using the same filter (weights) across different positions in the input image.

(-------------------------------------------------------------------------)

166- What is sparse connectivity in CNNs?

Ans- Sparse connectivity means each output neuron is connected to only a small region of the input, reducing computational complexity.

(-------------------------------------------------------------------------)

167- What is the role of the activation function in a CNN?

Ans- The activation function introduces non-linearity, allowing the CNN to model complex patterns.

(-------------------------------------------------------------------------)

168- Why is the ReLU activation function commonly used in CNNs?

Ans- ReLU (Rectified Linear Unit) introduces non-linearity while being computationally efficient and helping mitigate the vanishing gradient problem.

(-------------------------------------------------------------------------)

169-What is meant by "kernel size" in a convolutional layer?

Ans- Kernel size refers to the dimensions of the filter used during convolution, commonly 3x3 or 5x5 in CNNs.

(-------------------------------------------------------------------------)

170- What are some common loss functions used in CNNs?

Ans- Common loss functions include cross-entropy loss for classification tasks and mean squared error for regression tasks.

(-------------------------------------------------------------------------)

171- What is batch normalization?

Ans- Batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation, stabilizing learning.

(-------------------------------------------------------------------------)

172- What is the purpose of flattening in a CNN?

Ans- Flattening converts the 2D feature maps into a 1D vector, which can be fed into a fully connected layer for classification.

(-------------------------------------------------------------------------)

173- What is a typical use case for CNNs?

Ans- CNNs are commonly used in image classification, object detection, and segmentation tasks.

(-------------------------------------------------------------------------)

174- What is the function of a softmax layer in a CNN?

Ans- The softmax layer converts logits into probabilities, enabling multi-class classification.

(-------------------------------------------------------------------------)

175- What is an epoch in CNN training?

Ans- An epoch is one complete pass of the training dataset through the neural network.

(-------------------------------------------------------------------------)

176- Why is data augmentation used in CNN training?

Ans- Data augmentation increases the diversity of the training data by applying transformations like rotation, scaling, or flipping, improving model generalization.

(-------------------------------------------------------------------------)

177- What is transfer learning in CNNs?

Ans- Transfer learning involves using a pre-trained CNN on a new, similar task, reducing the amount of data and time required for training.

(------------------------------------------------------------------------)

178- What is the role of learning rate in CNN training?

Ans- The learning rate controls the size of the steps taken during gradient descent optimization.

(------------------------------------------------------------------------)

179- What is a learning rate scheduler?

Ans- A learning rate scheduler adjusts the learning rate during training to improve convergence and prevent overfitting.

(------------------------------------------------------------------------)

180- What is the difference between 1D, 2D, and 3D convolutions?

Ans- 1D convolutions are used for temporal data, 2D for images, and 3D for volumetric data like medical scans.

(------------------------------------------------------------------------)

181- What is a residual connection in CNNs?

Ans- Residual connections, used in ResNets, bypass one or more layers by adding the input to the output, helping to train deeper networks.

(-------------------------------------------------------------------------)

182- What is an Inception module?

Ans- The Inception module is a network architecture that uses multiple convolutional filters of different sizes simultaneously, enabling multi-scale feature extraction.

(-------------------------------------------------------------------------)

183- How does the VGGNet architecture differ from others?

Ans- VGGNet uses small 3x3 convolutional filters stacked deep to capture complex patterns, resulting in a large network with uniform layer sizes.

(-------------------------------------------------------------------------)

184- How do CNNs handle varying image sizes?

Ans- CNNs use pooling and global average pooling to reduce the spatial dimensions, making the network adaptable to different input sizes.

(-------------------------------------------------------------------------)

185- What is the difference between a CNN and an RNN?

Ans- CNNs are designed for spatial data like images, while RNNs are used for sequential data like time series or text.

(-------------------------------------------------------------------------)

186- How do you handle overfitting in CNN models?

Ans- Overfitting can be handled using techniques like dropout, data augmentation, regularization, and early stopping.

(-------------------------------------------------------------------------)

187- What is early stopping in CNN training?

Ans- Early stopping is a technique where training is halted when the model's performance on a validation set starts to degrade, preventing overfitting.

(-------------------------------------------------------------------------)

188- What is the role of the ImageNet dataset in CNNs?

Ans- ImageNet is a large dataset used for benchmarking CNN models and pre-training them for various computer vision tasks.

(-------------------------------------------------------------------------)

189- What is GoogLeNet?

Ans- GoogLeNet is a deep convolutional neural network architecture introduced by Google that won the ImageNet 2014 challenge.

(-------------------------------------------------------------------------)

190- What makes GoogLeNet different from previous architectures like AlexNet and VGG?

Ans- GoogLeNet is more efficient, with fewer parameters (7 million), and it uses an Inception module to handle different scales of information in parallel.

(-------------------------------------------------------------------------)

191- Why is GoogLeNet named as such?

Ans- GoogLeNet is named as a tribute to LeNet, one of the earliest CNN architectures.

(-------------------------------------------------------------------------)

192- What problem does the Inception architecture solve?

Ans- It addresses the need for deeper networks without drastically increasing computational cost or encountering issues like overfitting and vanishing gradients.

(-------------------------------------------------------------------------)

193- What is the fundamental building block of the Inception architecture?

Ans- The Inception Module, which applies multiple convolution filters of different sizes in parallel.

(-------------------------------------------------------------------------)

194- How does the Inception module mitigate the vanishing gradient problem?

Ans- By using parallel operations at multiple scales, reducing the depth required for complex feature extraction.

(-------------------------------------------------------------------------)

195- What operations are performed within an Inception module?

Ans- A 1x1 convolution, 3x3 convolution, 5x5 convolution, and a 3x3 max pooling, all in parallel.

(-------------------------------------------------------------------------)

196- How does the 1x1 convolution contribute to the Inception module?

Ans- It reduces the dimensionality of the feature maps, thereby decreasing the computational load.

(-------------------------------------------------------------------------)

197- What is the purpose of adding a 1x1 convolution after max pooling in the Inception module?

Ans- It reduces the output features of the max pooling before concatenation, maintaining computational efficiency.

(-------------------------------------------------------------------------)

198- Why is ReLU activation used after every convolution in the Inception module?

Ans- To introduce non-linearity, which allows the model to learn more complex representations.

(-------------------------------------------------------------------------)

199- What are auxiliary classifiers in GoogLeNet?

Ans- They are small classifiers branched from intermediate layers to provide additional gradient flow during training, helping to combat the vanishing gradient problem.

(-------------------------------------------------------------------------)

200- How do auxiliary classifiers contribute during training?

Ans- They provide additional loss functions that ensure the early layers receive meaningful gradients.

(-------------------------------------------------------------------------)

201- What happens to auxiliary classifiers during inference?

Ans- They are removed during inference, as their purpose is only to aid training.

(-------------------------------------------------------------------------)

202- What kind of pooling is used in GoogLeNet instead of fully connected layers?

Ans- Average pooling is used to reduce the feature maps along the spatial dimensions, minimizing the need for fully connected layers.

(-------------------------------------------------------------------------)

203- Why does GoogLeNet still include a fully connected layer despite using average pooling?

Ans- To slightly improve top-1 accuracy, though it contributes minimally to the total parameter count.

(-------------------------------------------------------------------------)

204- How many Inception modules are there in GoogLeNet?

Ans- There are nine Inception modules in the GoogLeNet architecture.

(-------------------------------------------------------------------------)

205- How does GoogLeNet handle downsampling in the network?

Ans- Downsampling is handled using max pooling layers after certain Inception blocks.

(-------------------------------------------------------------------------)

206- What is the role of Local Response Normalization (LRN) in GoogLeNet?

Ans- LRN is used to normalize feature maps, helping the network to converge during training.

(-------------------------------------------------------------------------)

207- How does GoogLeNet's average pooling layer differ from traditional pooling layers?

Ans- GoogLeNet uses global average pooling, which pools across the entire spatial dimensions of the feature map.

(-------------------------------------------------------------------------)

208- What activation function is used in GoogLeNet?

Ans- ReLU (Rectified Linear Unit) activation function.

(-------------------------------------------------------------------------)

209- What dropout rate is used in the auxiliary classifiers of GoogLeNet?

Ans- A dropout rate of 0.7 is used.

(-------------------------------------------------------------------------)

210- What is the final output size of the fully connected layer in GoogLeNet?

Ans- The final fully connected layer outputs a vector of size 1000, corresponding to the number of classes in ImageNet.

(-------------------------------------------------------------------------)






